<html>
<head>

<link rel="shortcut icon" href="images/icon.ico">

<!-- MathJax for LaTeX rendering -->
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    svg: { fontCache: 'global' }
  };
</script>
<script src="js/mathjax/tex-svg.js"></script>

<style type="text/css">
  body {
    background-color: #f5f9ff;
  }

  /* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

  .content-margin-container {
    display: flex;
    width: 100%;
    justify-content: left;
    align-items: center;
  }
  .main-content-block {
    width: 70%;
    max-width: 1100px;
    background-color: #fff;
    border-left: 1px solid #DDD;
    border-right: 1px solid #DDD;
    padding: 8px 24px 16px 24px;
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    box-shadow: 0 4px 8px rgba(0,0,0,0.03);
  }
  .margin-left-block {
    font-size: 14px;
    width: 15%;
    max-width: 130px;
    position: relative;
    margin-left: 10px;
    text-align: left;
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    padding: 5px;
  }
  .margin-right-block {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-size: 14px;
    width: 25%;
    max-width: 256px;
    position: relative;
    text-align: left;
    padding: 10px;
  }

  img {
    max-width: 100%;
    height: auto;
    display: block;
    margin: auto;
  }
  .conf-img {
      width: 420px;      /* or any width you choose */
      height: auto;
  }

  .roc-img {
      width: 420px;      /* match conf width */
      height: auto;
      margin-top: 10px;
  }

  .my-video {
    max-width: 100%;
    height: auto;
    display: block;
    margin: auto;
  }
  /* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

  a:link,a:visited {
    color: #0e7862;
    text-decoration: none;
  }
  a:hover {
    color: #24b597;
  }

  h1 {
    font-size: 22px;
    margin-top: 12px;
    margin-bottom: 10px;
  }

  table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
    width: 70%;
    max-width: calc(100% - 290px);
  }
  table td, table td * {
    vertical-align: middle;
    position: relative;
  }
  table.paper-code-tab {
    flex-shrink: 0;
    margin-left: 8px;
    margin-top: 8px;
    padding: 0px 0px 0px 8px;
    width: 290px;
    height: 150px;
  }

  .layered-paper {
    box-shadow:
      0px 0px 1px 1px rgba(0,0,0,0.35),
      5px 5px 0 0px #fff,
      5px 5px 1px 1px rgba(0,0,0,0.35),
      10px 10px 0 0px #fff,
      10px 10px 1px 1px rgba(0,0,0,0.35);
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

  div.hypothesis {
    width: 80%;
    background-color: #EEF6FF;
    border: 1px solid #C6D8FF;
    border-radius: 10px;
    font-family: Courier, monospace;
    font-size: 18px;
    text-align: center;
    margin: auto;
    padding: 16px 16px 16px 16px;
  }

  div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
    height: 200px;
  }

  .fade-in-inline {
    position: absolute;
    text-align: center;
    margin: auto;
    -webkit-mask-image: linear-gradient(to right,
                                        transparent 0%,
                                        transparent 40%,
                                        black 50%,
                                        black 90%,
                                        transparent 100%);
    mask-image: linear-gradient(to right,
                                transparent 0%,
                                transparent 40%,
                                black 50%,
                                black 90%,
                                transparent 100%);
    -webkit-mask-size: 8000% 100%;
    mask-size: 8000% 100%;
    animation-name: sweepMask;
    animation-duration: 4s;
    animation-iteration-count: infinite;
    animation-timing-function: linear;
    animation-delay: -1s;
  }

  .fade-in2-inline {
    animation-delay: 1s;
  }

  .inline-div {
    position: relative;
    display: inline-block;
    vertical-align: top;
    width: 50px;
  }

  .algorithm-container {
    max-width: 700px;
    margin: 8px auto 0 auto;
    padding: 16px 18px;
    background-color: #f5f7fb;
    border: 1px solid #d0d7f2;
    border-radius: 8px;
    text-align: left;
    display: block;
    box-shadow: 0 3px 6px rgba(0,0,0,0.04);
  }
  .algorithm-title {
    font-weight: bold;
    margin-bottom: 10px;
    font-size: 16px;
  }
  .code {
    font-family: "Courier New", Courier, monospace;
    background: #fdfdff;
    padding: 10px 12px;
    border-radius: 6px;
    border: 1px solid #e1e4f2;
    text-align: left;
    display: block;
    overflow-x: auto;
    font-size: 14px;
    line-height: 1.4;
  }

  /* Make LaTeX equations seamless (no boxes) */
  .formula {
    font-family: "Courier New", monospace;
    background-color: transparent;
    border: none;
    padding: 0;
    margin: 10px 0;
    display: block;
    text-align: center;
  }

  li {
    margin-bottom: 12px;
  }
  ol li ol li {
    margin-bottom: 8px;
  }

  /* New: side-by-side X-ray pair with equal vertical size */
  .xray-row {
    display: flex;
    justify-content: center;
    align-items: center;
    gap: 20px;
  }
  .xray-col {
    flex: 0 0 auto;
    text-align: center;
    max-width: 45%;
  }
  .xray-pair-img {
    height: 260px;
    width: auto;
    object-fit: contain;
    display: block;
    margin: 0 auto 6px auto;
  }
  .results-header {
  margin-top: 0;
  font-size: 20px;
  font-weight: 700;
  color: #2a2a2a;
}

.results-divider {
  margin: 6px 0 12px 0;
  border: none;
  border-top: 1px solid #bbb;
}

.results-pre {
  font-size: 12px;
  background: #fafafa;
  border: 1px solid #ddd;
  padding: 10px;
  border-radius: 6px;
  margin-top: 10px;
  white-space: pre;
}
.results-table {
width: 100%;
border-collapse: collapse;
font-size: 13px;
margin-top: 16px;
}
.results-table th,
.results-table td {
border: 1px solid #ddd;
padding: 6px 8px;
text-align: center;
}
.results-table th {
background-color: #f2f4ff;
font-weight: 600;
}
.results-table caption {
caption-side: top;
text-align: left;
font-weight: 600;
margin-bottom: 6px;
}


</style>

<title>Diagnosing the Imbalance: optimizing Vision Transformers for Pneumonia Detection in Chest X-Rays</title>
<meta property="og:title" content="Diagnosing the Imbalance: optimizing Vision Transformers for Pneumonia Detection in Chest X-Rays" />
<meta charset="UTF-8">
</head>

<body>

  <!-- HEADER -->
  <div class="content-margin-container">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <table class="header" align="left">
        <tr>
          <td colspan="4">
            <span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">
              Diagnosing the Imbalance: Optimizing Vision Transformers for Pneumonia Detection in Chest X-Rays
            </span>
          </td>
        </tr>
        <tr>
          <td align="left">
            <span style="font-size:17px"><a href="https://github.com/lawrencetang20">Lawrence Tang</a></span>
          </td>
          <td align="left">
            <span style="font-size:17px"><a href="#">Cole Foster</a></span>
          </td>
          <td align="left">
            <span style="font-size:17px"><a href="#">Chase Rubin</a></span>
          </td>
        <tr>
          <td colspan="4" align="left"><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
        </tr>
      </table>
    </div>
    <div class="margin-right-block">
    </div>
  </div>

  <!-- OUTLINE COLUMN -->
  <div class="content-margin-container" id="intro">
    <div class="margin-left-block">
      <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
        <b style="font-size:16px">Outline</b><br><br>
        <a href="#intro">Introduction</a><br><br>
        <a href="#prior_work">Prior Work</a><br><br>
        <a href="#methods_experiments">Methods &amp; Experiments</a><br><br>
        <a href="#results">Results</a><br><br>
        <a href="#implementation">Technical Implementation Details</a><br><br>
        <a href="#discussion">Discussion</a><br><br>
        <a href="#conclusion">Conclusion</a><br><br>
        <a href="#citations">References</a><br><br>
      </div>
    </div>
    <div class="main-content-block">
    </div>
    <div class="margin-right-block">
    </div>
  </div>

  <!-- SIDE-BY-SIDE EXAMPLE X-RAYS -->
  <div class="content-margin-container">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <div class="xray-row">
        <div class="xray-col">
          <img class="xray-pair-img" src="./images/NoPneumonia_Ex.png"
               alt="Normal chest X-ray (no pneumonia)" />
          <div style="font-size:14px;">No pneumonia</div>
        </div>
        <div class="xray-col">
          <img class="xray-pair-img" src="./images/Pneumonia_Ex.png"
               alt="Pneumonia-positive chest X-ray" />
          <div style="font-size:14px;">Pneumonia</div>
        </div>
      </div>
    </div>
    <div class="margin-right-block">
      <b>Figure 1.</b> Example chest X-rays: a normal study (left) and a pneumonia-positive study (right) from the RSNA dataset. The goal is to assign higher probability to the pneumonia class when subtle opacities are present.
    </div>
  </div>

  <!-- INTRODUCTION -->
  <div class="content-margin-container" id="intro">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <h1>Introduction</h1>

      <p>
        Deep learning systems that are deployed in medicine often face the worst possible combination of conditions, including noisy labels, strong distribution shifts, and severe class imbalance. The fraction of studies of confirmed pneumonia is far smaller than the number of benign scans in chest X-ray screening. However, the primary goal from a clinical perspective is to correctly identify these minority cases where the illness is existent.
      </p>
      <p>
        From a deep learning perspective, this raises a concrete question: when we deploy high-capacity architectures such as Vision Transformers in this setting, is their failure to detect rare disease cases primarily due to the backbone, or to how we train them under imbalance? Our working hypothesis is that, for a fixed ViT architecture, <i>the choice of loss and sampling strategy alone</i> can move a model from “looking good on paper” (high accuracy, nearly zero sensitivity) to a clinically useful operating point that trades a small increase in false positives for a large gain in pneumonia recall.
      </p>

      Thus, in this project we ask the following question:
      <div class="hypothesis" style="margin-top:16px; margin-bottom:16px;">
        <b>Research question.</b>  For a fixed Vision Transformer (ViT) architecture on a real, imbalanced medical imaging dataset, how do different loss functions and sampling strategies trade off overall accuracy compared to minority-class performance?
      </div>

      <p>
        The rest of this blog first reviews prior work on ViTs and imbalance handling, then details our experimental setup, compares six imbalance strategies on a shared ViT backbone, and closes with a discussion of clinical implications and limitations.
      </p>

      <p>
        By treating pneumonia versus no-pneumonia as a binary classification problem, we fine-tune an ImageNet-pretrained ViT-B/16 on the RSNA Pneumonia Detection Challenge chest X-ray dataset. From a basic analysis, we confirm that the dataset is moderately imbalanced: roughly 75% of the data is labelled as “no pneumonia,” with the remaining minority being pneumonia-positive images. We keep the architecture and data splits fixed and vary only how we handle class imbalance during training. We considered the following strategies throughout the project: 
      </p>
      <ul>
        <li>no imbalance handling (plain NLL loss),</li>
        <li>oversampling with a weighted sampler,</li>
        <li>stronger data augmentation for the minority class,</li>
        <li>class-weighted NLL loss,</li>
        <li>standard Focal Loss</li>
        <li>CAF-style mixed loss that blends class-weighted NLL with Focal Loss.</li>
      </ul>
      <p>
        By running all methods in a shared codebase on the same dataset, we aim to understand whether more complicated losses like Focal Loss and CAF-style mixes actually help compared to simpler tricks like oversampling or data augmentation. We wanted to isolate how loss and data rebalancing in an imbalanced setting can alter ViT behavior. We wanted to focus on metrics that are particularly relevant for screening such as accuracy, ROC-AUC, and sensitivity (minority-class recall).
      </p>
      <p>
        Our study is not about beating state-of-the-art on RSNA, but about treating loss shaping and sampling as a “knob” on top of a fixed ViT backbone and asking what actually changes in a clinically meaningful way. Concretely, our contributions are:
      </p>
      <ol>
        <li>
          <b>A controlled comparison of imbalance strategies on a ViT backbone.</b>
          We hold the architecture, optimizer, and data splits fixed and systematically vary only loss-level and data-level imbalance handling. This isolates how each strategy reshapes the confusion matrix and ROC curve, rather than conflating improvements with architectural changes.
        </li>
        <li>
          <b>A CAF-style mixed loss tailored to medical class imbalance.</b>
          We adapt class-adaptive focal ideas to a simple, ViT-compatible mixed objective that linearly combines class-weighted NLL and Focal Loss via a single mixing parameter λ, and we empirically show how this changes minority sensitivity and F1 relative to simpler baselines.
        </li>
        <li>
          <b>Clinically oriented analysis of “good” performance.</b>
          We show that superficially high accuracy can correspond to almost zero pneumonia recall, and we argue—through confusion matrices and per-class metrics—that rebalancing and focal-style objectives are better aligned with screening needs than raw accuracy or ROC-AUC alone.
        </li>
      </ol>
    </div>
    <div class="margin-right-block">
      The original project motivation was melanoma detection on under-represented skin types, but for compute and data-access reasons we prototype on the RSNA pneumonia dataset. The mechanics of imbalance and minority-class errors are similar, and the code and experimental setup can be reused for melanoma in future work.
    </div>
  </div>

    <!-- PRIOR WORK -->
  <div class="content-margin-container" id="prior_work">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <h1>Prior Work</h1>

      <b>Vision Transformers in medical imaging.</b><br>
      <p>
        Vision Transformers have become competitive alternatives to CNNs for image classification. ViT-B/16 divides an image into non-overlapping patches, embeds each patch, adds positional encodings, and feeds these tokens through a transformer encoder with multi-head self-attention before a classification head produces logits. While most clinical imaging work still relies on CNNs, several recent papers have shown that ViTs pre-trained on large natural image datasets can transfer well to radiology tasks like chest X-ray abnormality detection.<a href="#ref_1">[1]</a>
      </p>
      <br>

      <b>Class imbalance and reweighting.</b><br>
      <p>
        Class imbalance is a common issue in both vision and medical machine learning. The simplest approach to dealing with this imbalance is to reweight the loss by inverse class frequency–for a binary label y in {0,1} and predicted probability p, weighted NLL loss increases the penalty for misclassifying minority-class examples by assigning them higher weights. In the case of pneumonia detection, this corresponds to the idea that we’d prefer a healthy patient to be categorized as having pneumonia as opposed to a sick patient being classified as being healthy. These weights are passed directly into NLL Loss so that minority-class errors are penalized more heavily.<a href="#ref_4">[4]</a>
      </p>
      <div class="formula">
        \[
              w_c \propto \frac{1}{\text{count}(c)},
        \]
        \[
          L_{\text{wNLL}} = -\, w_1\, y \,\log(p)\;-\; w_0\, (1 - y)\, \log(1 - p)
        \]
      </div>
      <br>

      <b>Focal Loss.</b><br>
      <p>
        Focal Loss was introduced as a way to down-weight easy examples and focus training on hard, misclassified examples. When the parameter gamma is zero, Focal Loss is equivalent to weighted cross-entropy. For gamma greater than zero, easy examples with predicted probability close to one contribute less to the gradient, allowing the model to focus on harder cases.<a href="#ref_2">[2]</a>
      </p>
      <div class="formula">
        \[
          L_{\text{focal}} = -\, \alpha_y \, (1 - p_{\text{true}})^{\gamma} \, \log\!\bigl(p_{\text{true}}\bigr)
        \]
      </div>
      <br>

      <b>Class-adaptive focal variants.</b><br>
      <p>
        Some works propose making gamma class-dependent, so that rare classes receive both higher weight and stronger focusing. We follow this spirit by fixing a single focusing parameter and mixing it with a class-weighted NLL term, allowing the degree of emphasis on rare classes to be controlled by a single hyperparameter lambda.
      </p>
      <br>

      <b>Medical imaging benchmarks.</b><br>
      <p>
        The RSNA Pneumonia Detection Challenge dataset is an X-ray benchmark with image-level labels derived from radiology reports. Unlike toy datasets where classes are balanced by design, RSNA has a more realistic clinical label distribution, making it useful for studying imbalance in a setting that resembles real screening workflows. Most prior work on class imbalance in medical imaging reports results for one or two tricks, like class weights or Focal Loss, but rarely compares them in a controlled setting. Our project makes that comparison explicit for ViTs.<a href="#ref_3">[3]</a>
      </p>
    </div>
    <div class="margin-right-block">
      Most prior work on class imbalance in medical imaging reports results for one or two tricks (e.g., class weights or focal loss) but rarely compares them in a controlled setting on the same backbone. Our project tries to make that comparison explicit for ViTs.
    </div>
  </div>


  <!-- ViT ARCHITECTURE FIGURE -->
  <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <img src="./images/ViT.png" alt="Vision Transformer architecture diagram" />
    </div>
    <div class="margin-right-block">
      <b>Figure 2.</b> Schematic of the Vision Transformer (ViT-B/16) architecture used in our experiments. We fine-tune all layers and replace the final classification head with a 2-class log-probability head for pneumonia vs. no pneumonia.
    </div>
  </div>

    <!-- METHODS & EXPERIMENTS -->
  <div class="content-margin-container" id="methods_experiments">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <h1>Methods &amp; Experiments</h1>

      <b>Dataset and splits.</b><br>
      <p>
        We use the Kaggle RSNA Pneumonia Detection Challenge dataset via kagglehub.<a href="#ref_3">[3]</a> Each DICOM study is associated with one or more bounding boxes and a binary label “Target” in {0,1} indicating the presence of pneumonia. We convert this to binary classification task by grouping rows by patientId and setting a patient-level target equal to the maximum of the per-row Target values. Finally, we construct a single DICOM path per patient, filtering to images that actually exist on disk.
      </p>

      <p>
        We then perform dataset splits at the patient level, ensuring class proportions are preserved within each dataset. First, we create a 75 percent train and 25 percent validation split, stratified by the target. We use 10 percent of the training portion as an internal test set, again stratified. This gives three disjoint sets with similar positive/negative ratios. It is important to note we do not perform any manual downsampling, meaning the imbalance present in the raw data is preserved.
      </p>
      <br>

      <b>Transforms and DICOM handling.</b><br>
      <p>
        For preprocessing and transforms, each DICOM file is read with pydicom, normalized to the range [0, 255], and converted to a 3-channel PIL image so that we can reuse ImageNet normalization and ViT preprocessing. We define a base train transform that resizes to 224×224, applies random horizontal flip, converts to a tensor, and applies ImageNet mean/variance normalization. We define a minority train transform that starts from the base transform and adds random vertical flip, random rotation, and ColorJitter for brightness, contrast, saturation, and hue. For evaluation, we use an eval transform that resizes to 224×224, converts to a tensor, and normalizes, but does not use stochastic augmentation. The custom RsnaPneumoniaDataset chooses between the base and minority transform dynamically depending on the label and whether “augment minority” is enabled in a particular experiment.
      </p>
      <br>

      <b>Model.</b><br>
      <p>
        For the model, we fine-tune a ViT-B/16 model from torchvision. We initialize with ImageNet-1K pretrained weights <a href="#ref_1">[1]</a>, replace the classification head with a 2-class linear layer followed by a LogSoftmax over the two classes (pneumonia and no pneumonia), and we train all layers without freezing the encoder parameters.
      </p>
      <br>

      <b>Loss functions.</b><br>
      <p>
        All of the experiments ran use log-probabilities inputs to the loss function. We consider four different loss functions.
      </p>

      <ol>

        <!-- PLAIN NLL -->
        <li>
          <b>Plain NLL Loss.</b> Standard NLL with no class weighting.
        </li>

        <!-- CLASS-WEIGHTED NLL -->
        <li>
          <b>Class-weighted NLL Loss.</b><br>
          We compute class counts on the training set and define approximate inverse-frequency weights, rescaled so that the mean weight over classes is about one, and pass these to NLL Loss so that positive (pneumonia) examples contribute more to the loss.
          <div class="formula">
            \[
              w_c \propto \frac{1}{\text{count}(c)},
            \]
            \[
              L_{\text{wNLL}}
              = -\, w_1\, y\, \log(p)
              \;-\;
              w_0\, (1 - y)\, \log(1 - p)
            \]
          </div>
        </li>

        <!-- FOCAL LOSS -->
        <li>
          <b>Focal Loss on log-probabilities.</b><br>
          We implement a multi-class Focal Loss that extracts the predicted probability of the true class and applies a focusing term with gamma = 2.0 and class-balancing α = [0.25, 0.75].<a href="#ref_2">[2]</a>
          <div class="formula">
            \[
              L_{\text{focal}}
              = -\, \alpha_y \,
              (1 - p_{\text{true}})^{\gamma}
              \log(p_{\text{true}})
            \]
          </div>
        </li>

        <!-- CAF MIXED LOSS -->
        <li>
          <b>CAF-style mixed loss: class-weighted NLL + Focal.</b><br>
          We combine class-weighted NLL and Focal Loss in a single mixed objective of the form (1 − lambda) times the weighted NLL plus lambda times the focal loss. Here, the weighted NLL uses the same class weights as before, and the Focal Loss uses the same alpha and gamma as above. In our experiments we fix lambda equal to 0.8 so that rare classes receive both higher loss weight and stronger focusing.

          <div class="formula">
            \[
              L_{\text{CAF}}
              = (1 - \lambda)\, L_{\text{wNLL}}
              + \lambda\, L_{\text{focal}}
            \]
          </div>
        </li>

      </ol>
      <br>

      <b>Data-level imbalance strategies.</b><br>
      <p>In addition to loss shaping, we explore two data-level strategies:</p>

      <ol>
        <li>
          <b>WeightedRandomSampler.</b><br>
          In this method, for a given training dataframe, we build a vector of sample weights inversely proportional to class frequency and use PyTorch’s WeightedRandomSampler to draw balanced mini-batches. The underlying dataset is unchanged, but the sampling distribution changes. The second strategy is targeted augmentation of the minority class.
        </li>

        <li>
          <b>Targeted minority augmentation.</b><br>
          Our baseline ViT model already applies some basic augmentations to all training images such as resizing and random horizontal flips. In order to further boost diversity within the minority pneumonia class, we introduced a stronger augmentation for only these minority datapoints. In addition to the baseline transforms, minority examples received vertical flips, small-angle rotations, and color jitter. These augmentations are added with the hope that the model learns more robust and discriminative features rather than memorizing the few minority samples.
        </li>
      </ol>
      <br>

      <b>Experimental design and baselines.</b><br>
      <p>
        We deliberately keep the experimental design minimalistic in order to stress-test loss shaping and sampling rather than architecture search. All methods share the same ViT-B/16 backbone, preprocessing pipeline, optimizer (Adam, learning rate 3e-4), batch size, number of epochs, and train/val/test splits. The <code>baseline_ViT</code> configuration acts as a “do nothing” reference for how a standard ViT behaves under raw class imbalance. Each subsequent configuration changes exactly one dimension at a time—either data-level sampling (<code>oversample_minority</code>, <code>augment_minority</code>) or loss-level weighting (<code>class_weighted_NLL</code>, <code>focal_loss</code>, <code>caf_nll</code>)—so that relative differences can be attributed to that intervention rather than confounded by unrelated hyperparameter changes. All experiments share the same seed, data splits, model architecture, optimizer, and 15 training epochs. The configurations we run are:
      </p>

      <ol>
        <li> <b>baseline_ViT</b>: plain ViT with NLL; no oversampling; base augmentation only.</li>
        <li> <b>oversample_minority</b>: NLL + WeightedRandomSampler.</li>
        <li> <b>augment_minority</b>: NLL + stronger augmentation for pneumonia images only.</li>
        <li> <b>class_weighted_NLL</b>: class-weighted NLL with inverse-frequency weights.</li>
        <li> <b>focal_loss</b>: Focal Loss with γ = 2.0 and α = [0.25, 0.75].</li>
        <li> <b>caf_nll</b>: CAF-style mixed loss with λ = 0.8.</li>
      </ol>

    </div>

    <div class="margin-right-block" style="transform: translate(0%, -100%);">
      The experiments are intentionally simple: we keep the backbone fixed and focus exclusively on how loss shaping and sampling change the confusion matrix and ROC curve.
    </div>

  </div>



    <!-- RESULTS -->
  <div class="content-margin-container" id="results">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <h1>Results</h1>

      <p>
        At a high level, our experiments support three qualitative patterns:
      </p>
      <ul>
        <li><b>Uncorrected ViT collapses to the majority class.</b> The baseline model achieves decent accuracy but almost never predicts pneumonia, illustrating how misleading aggregate metrics can be under imbalance.</li>
        <li><b>Simple rebalancing already changes the regime.</b> Oversampling and class-weighted NLL shift the model into a regime where pneumonia recall is non-trivial, at the cost of more false positives.</li>
        <li><b>CAF-style mixing refines this trade-off.</b> Focal Loss and the CAF mixed loss further improve minority F1 while keeping accuracy high, suggesting that modestly more complex objectives can yield better screening behavior without altering the backbone.</li>
      </ul>

      <p>
        We summarize performance for each imbalance strategy on the held-out test set. For each experiment we save a confusion matrix and the ROC curve and report key metrics.
      </p>

      <ul>
        <li><b>baseline_ViT</b>: achieves fairly high overall accuracy but almost never predicts pneumonia, leading to extremely low minority recall</li>

        <li><b>oversample_minority</b>:  improves pneumonia recall (from roughly 1 percent to roughly 68 percent), but results in more false positives and a slight drop in accuracy.</li>

        <li><b>augment_minority</b>: the augment minority configuration alone does not fix the bias as the model still collapses to predicting “no pneumonia” for almost every image in the test set.</li>

        <li><b>class_weighted_NLL</b>: meaningfully improves pneumonia recall while preserving high accuracy on the majority class.</li>

        <li><b>focal_loss</b>: further improves minority performance while keeping overall accuracy high, with a slight improvement in minority F1 compared to just class weighting.</li>

        <li><b>caf_nll</b>: has both high overall performance while further boosting pneumonia recall compared to plain weighting.</li>
      </ul>

    </div>

    <div class="margin-right-block">
      Figures 3–8 provide confusion matrices and metric blocks for each experiment. ROC curves are saved alongside the confusion matrices and can be displayed as paired subfigures if desired.
    </div>
  </div>

<!-- RESULTS: baseline_ViT -->
<div class="content-margin-container">
  <div class="margin-left-block"></div>

  <div class="main-content-block">

      <h2 class="results-header">Baseline Model: <code>baseline_ViT</code></h2>
      <hr class="results-divider">

      <img src="./images/Baseline_Conf.png" class="conf-img" alt="baseline_ViT confusion matrix" />
      <img src="./images/Baseline_ROC.png" class="roc-img" alt="baseline_ViT ROC curve" style="margin-top:10px;" />

      <pre class="results-pre">
FINAL TEST RESULTS: baseline_ViT
  Test loss 0.4647 acc 0.7757

Test Confusion Matrix (raw):
 [[1550    1]
 [ 448    3]]

Test Classification Report:
                  precision    recall  f1-score   support

no_pneumonia(0)       0.78      1.00      0.87      1551
   pneumonia(1)       0.75      0.01      0.01       451

       accuracy                           0.78      2002
      macro avg       0.76      0.50      0.44      2002
   weighted avg       0.77      0.78      0.68      2002
      </pre>
  </div>

  <div class="margin-right-block">
    <b>Figure 3.</b> Test confusion matrix for <code>baseline_ViT</code> (no explicit imbalance handling). The model almost never predicts pneumonia: minority recall is essentially zero despite a superficially reasonable overall accuracy.
  </div>
</div>

<!-- RESULTS: oversample_minority -->
<div class="content-margin-container">
  <div class="margin-left-block"></div>

  <div class="main-content-block">

      <h2 class="results-header">Oversampling Strategy: <code>oversample_minority</code></h2>
      <hr class="results-divider">

      <img src="./images/Oversample_Conf.png" class="conf-img" alt="oversample_minority confusion matrix" />
      <img src="./images/Oversample_ROC.png" class="roc-img" alt="oversample_minority ROC curve" style="margin-top:10px;" />

      <pre class="results-pre">
FINAL TEST RESULTS: oversample_minority
  Test loss 0.4989 acc 0.7657

Test Confusion Matrix (raw):
 [[1228  323]
 [ 146  305]]

Test Classification Report:
                  precision    recall  f1-score   support

no_pneumonia(0)       0.89      0.79      0.84      1551
   pneumonia(1)       0.49      0.68      0.57       451

       accuracy                           0.77      2002
      macro avg       0.69      0.73      0.70      2002
   weighted avg       0.80      0.77      0.78      2002
      </pre>
  </div>

  <div class="margin-right-block">
    <b>Figure 4.</b> Test confusion matrix for <code>oversample_minority</code> (WeightedRandomSampler). Oversampling boosts pneumonia recall dramatically but introduces more false positives among normal studies.
  </div>
</div>

<!-- RESULTS: augment_minority -->
<div class="content-margin-container">
  <div class="margin-left-block"></div>

  <div class="main-content-block">

      <h2 class="results-header">Additional Minority Augmentations: <code>augment_minority</code></h2>
      <hr class="results-divider">

      <img src="./images/Augment_Conf.png" class="conf-img" alt="augment_minority confusion matrix" />
      <img src="./images/Augment_ROC.png" class="roc-img" alt="augment_minority ROC curve" style="margin-top:10px;" />

      <pre class="results-pre">
FINAL TEST RESULTS: augment_minority
  Test loss 1.6580 acc 0.7732

Test Confusion Matrix (raw):
 [[1548    3]
 [ 451    0]]

Test Classification Report:
                  precision    recall  f1-score   support

no_pneumonia(0)       0.77      1.00      0.87      1551
   pneumonia(1)       0.17      0.00      0.00       451

       accuracy                           0.77      2002
      macro avg       0.47      0.50      0.44      2002
   weighted avg       0.64      0.77      0.68      2002
      </pre>
  </div>

  <div class="margin-right-block">
    <b>Figure 5.</b> Test confusion matrix for <code>augment_minority</code> (stronger transforms on pneumonia-only). Augmentation alone does not fix the bias: the model still predicts “no pneumonia” for essentially all positive cases.
  </div>
</div>

<!-- RESULTS: class_weighted_NLL -->
<div class="content-margin-container">
  <div class="margin-left-block"></div>

  <div class="main-content-block">

      <h2 class="results-header">Class Weighting: <code>class_weighted_NLL</code></h2>
      <hr class="results-divider">

      <img src="./images/ClassWeighted_Conf.png" class="conf-img" alt="class_weighted_NLL confusion matrix" />
      <img src="./images/ClassWeighted_ROC.png" class="roc-img" alt="class_weighted_NLL ROC curve" style="margin-top:10px;" />

      <pre class="results-pre">
FINAL TEST RESULTS: class_weighted_NLL
  Test loss 0.6264 acc 0.7967

Test Classification Report:
                  precision    recall  f1-score   support

no_pneumonia(0)       0.84      0.92      0.87      1551
   pneumonia(1)       0.57      0.38      0.46       451

       accuracy                           0.80      2002
      macro avg       0.70      0.65      0.67      2002
   weighted avg       0.78      0.80      0.78      2002
      </pre>
  </div>

  <div class="margin-right-block">
    <b>Figure 6.</b> Test confusion matrix for <code>class_weighted_NLL</code>. Inverse-frequency class weights meaningfully improve pneumonia recall while preserving high accuracy on the majority class.
  </div>
</div>

<!-- RESULTS: focal_loss -->
<div class="content-margin-container">
  <div class="margin-left-block"></div>

  <div class="main-content-block">

      <h2 class="results-header">Focal Loss: <code>focal_loss</code></h2>
      <hr class="results-divider">

      <img src="./images/Focal_Conf.png" class="conf-img" alt="focal_loss confusion matrix" />
      <img src="./images/Focal_ROC.png" class="roc-img" alt="focal_loss ROC curve" style="margin-top:10px;" />

      <pre class="results-pre">
FINAL TEST RESULTS: focal_loss
  Test loss 0.0549 acc 0.8127

Test Classification Report:
                  precision    recall  f1-score   support

no_pneumonia(0)       0.84      0.94      0.89      1551
   pneumonia(1)       0.64      0.39      0.48       451

       accuracy                           0.81      2002
      macro avg       0.74      0.66      0.68      2002
   weighted avg       0.79      0.81      0.80      2002
      </pre>
  </div>

  <div class="margin-right-block">
    <b>Figure 7.</b> Test confusion matrix for <code>focal_loss</code>. Focal Loss further sharpens minority performance while keeping overall accuracy high, at the cost of extra hyperparameters.
  </div>
</div>

<!-- RESULTS: caf_nll -->
<div class="content-margin-container">
  <div class="margin-left-block"></div>

  <div class="main-content-block">

      <h2 class="results-header">CAF Mixed Loss: <code>caf_nll</code></h2>
      <hr class="results-divider">

      <img src="./images/CAF_Conf.png" class="conf-img" alt="caf_nll confusion matrix" />
      <img src="./images/Caf_ROC.png" class="roc-img" alt="caf_nll ROC curve" style="margin-top:10px;" />

      <pre class="results-pre">
FINAL TEST RESULTS: caf_nll
  Test loss 0.2097 acc 0.8102

Test Confusion Matrix (raw):
 [[1407  144]
 [ 236  215]]

Test Classification Report:
                  precision    recall  f1-score   support

no_pneumonia(0)       0.86      0.91      0.88      1551
   pneumonia(1)       0.60      0.48      0.53       451

       accuracy                           0.81      2002
      macro avg       0.73      0.69      0.71      2002
   weighted avg       0.80      0.81      0.80      2002
      </pre>
  </div>

  <div class="margin-right-block">
    <b>Figure 8.</b> Test confusion matrix for <code>caf_nll</code> (mixed weighted NLL + Focal Loss, λ = 0.8). CAF provides a strong compromise, further improving pneumonia F1 while maintaining robust majority-class performance.
  </div>
</div>

<!-- RESULTS: summary table -->
<div class="content-margin-container">
  <div class="margin-left-block"></div>

  <div class="main-content-block">
    <h2 class="results-header">Summary of Test Performance Across Methods</h2>
    <hr class="results-divider">

    <table class="results-table">
      <caption>
        Best score per row is shown in <b>bold</b>. Approximate 95% confidence
        intervals from Hoeffding's inequality: accuracy ±0.03 (absolute),
        pneumonia recall ±0.06–0.07 (absolute).
      </caption>
      <thead>
        <tr>
          <th>Metric</th>
          <th><code>baseline_ViT</code></th>
          <th><code>oversample_minority</code></th>
          <th><code>augment_minority</code></th>
          <th><code>class_weighted_NLL</code></th>
          <th><code>focal_loss</code></th>
          <th><code>caf_nll</code></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Test loss (↓)</td>
          <td>0.4647</td>
          <td>0.4989</td>
          <td>1.6580</td>
          <td>0.6264</td>
          <td><b>0.0549</b></td>
          <td>0.2097</td>
        </tr>
        <tr>
          <td>Accuracy</td>
          <td>0.7757</td>
          <td>0.7657</td>
          <td>0.7732</td>
          <td>0.7967</td>
          <td><b>0.8127</b></td>
          <td>0.8102</td>
        </tr>
        <tr>
          <td>Pneumonia precision (class 1)</td>
          <td><b>0.75</b></td>
          <td>0.49</td>
          <td>0.17</td>
          <td>0.57</td>
          <td>0.64</td>
          <td>0.60</td>
        </tr>
        <tr>
          <td>Pneumonia recall (class 1)</td>
          <td>0.01</td>
          <td><b>0.68</b></td>
          <td>0.00</td>
          <td>0.38</td>
          <td>0.39</td>
          <td>0.48</td>
        </tr>
        <tr>
          <td>Pneumonia F1 (class 1)</td>
          <td>0.01</td>
          <td><b>0.57</b></td>
          <td>0.00</td>
          <td>0.46</td>
          <td>0.48</td>
          <td>0.53</td>
        </tr>
        <tr>
          <td>Macro precision</td>
          <td><b>0.76</b></td>
          <td>0.69</td>
          <td>0.47</td>
          <td>0.70</td>
          <td>0.74</td>
          <td>0.73</td>
        </tr>
        <tr>
          <td>Macro recall</td>
          <td>0.50</td>
          <td><b>0.73</b></td>
          <td>0.50</td>
          <td>0.65</td>
          <td>0.66</td>
          <td>0.69</td>
        </tr>
        <tr>
          <td>Macro F1</td>
          <td>0.44</td>
          <td>0.70</td>
          <td>0.44</td>
          <td>0.67</td>
          <td>0.68</td>
          <td><b>0.71</b></td>
        </tr>
      </tbody>
    </table>

      <p>
        Our test set has 2,002 patients (451 pneumonia-positive, 1,551 pneumonia-negative).
        For each model, we can treat correctness on each test example as a 0–1 variable and
        apply Hoeffding's inequality. Hoeffding's inequality implies that with 95% confidence,
        the true accuracy is within about ±3 percentage points of the reported test accuracy.
        Applying the same reasoning to pneumonia recall, using only the 451 positive patients,
        we get a 95% confidence band of roughly ±6–7 percentage points. While sampling noise
        can shift our pneumonia recall by several percentage points, the gains we see from
        about 1% recall for the baseline ViT to roughly 48–68% recall for the CAF and
        oversampling methods are far larger than this uncertainty.
      </p>
  </div>

  <div class="margin-right-block">
    <b>Table 1.</b> Side-by-side comparison of test metrics across all imbalance strategies. Bold entries denote the best metric value in each row.
  </div>
</div>


  <!-- IMPLEMENTATION DETAILS -->
  <div class="content-margin-container" id="implementation">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <h1>Technical Implementation Details</h1>
      <p>
        All experiments are implemented in a single Python notebook structured into three blocks. The first block contains data download, splits, and transforms. The second block has our model and loss definitions. Finally, the third block contains the training and all of the experiments.
      </p>

      <b>Training loop and evaluation.</b><br>
      We use Adam with a learning rate of 3e-4 for optimization, using a batch size of 32 and 15 epochs per experiment. For each experiment, we track validation accuracy over epochs and save the model checkpoint with the best validation accuracy. After training, we reload that checkpoint and compute test-set metrics, including a confusion matrix, classification report, and an ROC curve using scikit-learn. Our core training and evaluation functions are shown below:
      <div class="algorithm-container">
        <div class="algorithm-title">Algorithm 1: Training and Evaluation Loop</div>
        <div class="code">
train_one_epoch(model, loader, optimizer, criterion):<br>
&nbsp;&nbsp;model.train()<br>
&nbsp;&nbsp;for (imgs, labels) in loader:<br>
&nbsp;&nbsp;&nbsp;&nbsp;imgs, labels = imgs.to(device), labels.to(device)<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zero_grad()<br>
&nbsp;&nbsp;&nbsp;&nbsp;log_probs = model(imgs)  # (N, 2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss = criterion(log_probs, labels)<br>
&nbsp;&nbsp;&nbsp;&nbsp;loss.backward()<br>
&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<br>
<br>
evaluate(model, loader, criterion, return_scores):<br>
&nbsp;&nbsp;model.eval()<br>
&nbsp;&nbsp;for (imgs, labels) in loader:<br>
&nbsp;&nbsp;&nbsp;&nbsp;with torch.no_grad():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log_probs = model(imgs.to(device))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = criterion(log_probs, labels.to(device))<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;preds = argmax(log_probs, dim=1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if return_scores:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probs_pos = exp(log_probs)[:, 1]
        </div>
      </div><br>
      <b>Reproducibility.</b><br>
      <p>
        For reproducibility, we fix random seeds for PyTorch, NumPy, and Python’s <code>random</code> module, and we log all configuration choices (loss type, sampler type, augmentation flags) as named experiment tags. Each configuration is trained once with this fixed seed; although this does not capture training-seed variance, it ensures that differences between rows in Table&nbsp;1 reflect only the configured imbalance strategy rather than accidental changes in data ordering or initialization.
      </p>


    </div>
    <div class="margin-right-block">
      The full source code, including the CAF-style mixed loss implementation and experiment scripts, is packaged in the <code>code</code> folder.
    </div>
  </div>

  <!-- DISCUSSION -->
  <div class="content-margin-container" id="discussion">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <h1>Discussion</h1>

      <p>
        We wanted to focus on how each strategy could alter both the confusion matrix and ROC curve by looking at the balance between overall accuracy and minority-class performance. This is due to the fact that setting false negatives for pneumonia are much more costly than false positives in a screening.
      </p>

      <b>Baseline vs. naive rebalancing:</b><br>
      <p>
        We see that the baseline ViT with plain NLL achieves a seemingly high test accuracy of about 0.78. However this number is misleading because the confusion matrix reveals that the model almost never predicts pneumonia, predicting only three true positives out of 451 pneumonia cases. We can clearly see that the model has not been learning anything useful; it has actually learned that always predicting “no pneumonia” is an easy local optimum under class imbalance. Thus, this shows a common failure in medical machine learning where high accuracy is driven entirely by the majority class and is misleading.
      </p>

      <p>
        For oversampling with <code>WeightedRandomSampler</code>, we see this picture changes dramatically. We encourage the model to pay attention to minority examples during training by frequently sampling examples that are positive. The test confusion matrix shows that pneumonia recall jumps to 68 percent, which is more meaningful than the baseline ViT. Now, we catch a majority of pneumonia cases. However, the cost of this is an increase in false positives and a small drop in overall accuracy from about 0.78 to about 0.77. From a screening perspective this trade-off is often acceptable, but oversampling alone can distort calibration and make a mismatch between the training and deployment prevalence.
      </p>

      <b>Augmenting the minority class in isolation:</b><br>
      <p>
        If we have stronger augmentation only for pneumonia-positive images (while minimally changing the sampling distribution), this should offer a more modest intervention and the model should see a more diverse view of each positive. However, in our experiments this does not solve the fundamental bias toward the majority class: the model still predicts no pneumonia for almost all pneumonia cases, with zero true positives on the test set. This shows that in this medical setting increasing exposure frequency of the minority class is more critical than diversity alone. Overall, augmentation is valuable, but it appears to work best when combined with a rebalancing of how often the minority class is sampled or how heavily its errors are weighted.
      </p>

      <b>Class-weighted NLL and Focal Loss:</b><br>
      <p>
        Class weighting directly encodes the idea that misclassifying a pneumonia-positive study should be penalized more heavily than misclassifying a normal study. In our class_weighted_NLL experiment, this leads to a more balanced confusion matrix: we see that pneumonia recall rises from around 1 percent in the baseline to around 38 percent, while accuracy also slightly increases to about 0.80. The model now behaves more like a screening system, as it catches a meaningful fraction of pneumonia cases but does not completely sacrifice specificity.
      </p>

      <p>
        For Focal Loss, we add a second layer of bias and focus training on hard examples by down-weighting easy, correctly classified cases. In the focal_loss experiment, we see that pneumonia recall is around the high 30s or 40 percent range, but there is also slightly improved overall accuracy (around 0.81) and a better balance between sensitivity and specificity. Qualitatively, Focal Loss shows that it can help the model pay attention to the borderline cases near the decision boundary and not overfitting to a few extremely easy positives. However, this comes at the cost of extra hyperparameters (alpha and gamma) that can be non-trivial to tune and may interact with dataset noise and label quality.
      </p>

      <b>CAF-style mixed loss:</b><br>
      <p>
        Our CAF-style loss was implemented as a mixture of class-weighted NLL and Focal Loss with mixing parameter lambda equal to 0.8. This mixing parameter was found through cross validation where we chose the value that maximized validation AUC from the set of candidate lambdas {0.2, 0.4, 0.6, 0.8} when we train on 5 epochs. This was meant to capture the intuition that we want both a prior that treats minority-class errors as more expensive and a focus on hard examples within each class, treating both ideas mentioned above. In the caf_nll experiment, we see that pneumonia recall rise to roughly 48 percent while maintaining an overall accuracy of about 0.81 and strong performance on the majority class (no-pneumonia recall around 91 percent). The pneumonia F1 score is also the highest among the reweighting-based methods. From a qualitative standpoint, we can see that CAF behaves like a balanced focal loss: it avoids some of the extreme over-correction of pure oversampling, yet still substantially improves minority recall relative to plain class weighting. Because the weighting and focusing are tied together with a single lambda, the effective behavior is easier to reason about than manually tuning independent alpha and gamma for each class. In practice, this can simplify the hyperparameter search for those that are working under tight compute budgets.
      </p>

      <b>ROC curves and operating points:</b><br>
      <p>
        Overall, ROC-AUC values remain in a relatively narrow band (around the low-to-mid 0.8s) across all experiments even when the confusion matrices look very different. This highlights a key lesson: for highly imbalanced clinical tasks, the ROC-AUC statistic alone can really obscure key differences in model behavior at the operating points a clinician might actually use. For example, baseline_ViT and class_weighted_NLL can have similar AUCs, but one completely fails to detect pneumonia while the other achieves non-trivial recall. Our results show the need to report per-class metrics and confusion matrices in addition to summary statistics like ROC-AUC.
      </p>
      <b>Loss Functions Implicitly Choose an Operating Point</b><br>
      <p>
        A key lesson from our experiments is that, with a fixed ViT backbone, the choice of training objective acts as an implicit decision about <i>where</i> on the ROC curve the model will operate. Under plain NLL on an imbalanced dataset, empirical risk minimization is effectively cost-insensitive: the model is rewarded for optimizing overall accuracy, which is dominated by the majority “no pneumonia” class. In this regime, the ViT converges to a degenerate operating point that almost never predicts pneumonia, achieving high accuracy but essentially zero sensitivity to the minority class.
      </p>

      <p>
        Once we introduce class-weighted NLL, oversampling, or focal-style objectives, we do not change the architecture, optimization algorithm, or data splits; we only change how the loss function scores mistakes. The confusion matrices in Figures 3–8 and the summary in Table&nbsp;1 show that these objectives move the ViT to a very different part of the ROC curve: pneumonia recall jumps from almost zero to 30–70% depending on the strategy, with only modest changes in overall accuracy and macro metrics. In other words, reweighting and focal-style losses behave like built-in, differentiable versions of threshold tuning for a cost-sensitive classifier.
      </p>

      <p>
        Conceptually, this suggests that for high-capacity models such as ViTs, imbalance handling is less about “fixing a weak backbone” and more about encoding an explicit preference over false negatives versus false positives into the training objective. The same architecture can look either useless or clinically plausible depending purely on whether the loss function is cost-insensitive (plain NLL) or cost-aware (class weighting, oversampling, CAF). This supports a broader view of deep learning under class imbalance: the loss function is not just a numeric objective, but an implicit choice of operating point and error costs.
      </p>

      <b>Methodological limitations:</b><br>
      <p>
        However, there are several limitations to our study. First, we work with a fixed ViT-B/16 backbone and a single set of preprocessing transforms in order to keep these uniform across all different loss functions and sampling techniques. There could be different architectures or stronger augmentations might change the relative ranking of imbalance strategies. Second, we run each experiment once with a fixed seed. A more thorough study could average over multiple runs and seeds to separate systematic effects from stochastic variation. We could not do this simply due to compute and time constraints. Third, we focus only on pneumonia versus no-pneumonia. In real chest X-ray workflows, models are often multi-label and must handle many findings simultaneously, which should create richer patterns of imbalance. We focused only on pneumonia simply, again, due to computer power as training a model with many different labels would take more time. Finally, we evaluate primarily on accuracy, recall, precision, F1, and ROC-AUC. There are calibration metrics such as Expected Calibration Error or Brier score that would give a bigger picture of how these loss functions affect probability estimates.
      </p>

      <b>Practical implications for practitioners.</b><br>
      <p>
        For practitioners deploying ViTs (or similar backbones) on imbalanced medical tasks, our results suggest a simple checklist: first, never rely on accuracy alone; always inspect per-class recall and confusion matrices. Second, start with class-weighted NLL or oversampling as low-complexity baselines that already fix a large fraction of the failure mode we observe in <code>baseline_ViT</code>. Third, if minority sensitivity remains unsatisfactory and computational budget allows, consider focal-style or CAF-style objectives to fine-tune the trade-off between missed cases and false alarms. Even in this modest student project setting, following that progression changes the model from “almost never flags pneumonia” to “misses about half as many positive cases,” which is a qualitatively different clinical behavior.
      </p>

      <p>
        Despite these caveats, the overall picture is consistent; we showed that simple reweighting and oversampling methods can offer large gains over the uncorrected baseline model, and focal-style objectives provide an additional improvement in the minority regime. Our CAF-style mixed loss appears to be a useful compromise for practitioners who want principled handling of class imbalance without an overly complicated loss design.
      </p>
      <b>Scope of claims.</b><br>
      <p>
        Taken together, we interpret our results as evidence about <i>relative</i> behavior of imbalance strategies on this specific ViT-B/16 + RSNA setup, not as a universal ranking of methods across all architectures or datasets. For example, it is possible that CNN-based backbones or multi-label chest X-ray tasks would reshuffle which method is “best.” Our main claim is therefore conceptual: that reweighting, sampling, and focal-style objectives can radically change minority sensitivity even when ROC-AUC and overall accuracy move only modestly, and that CAF-style mixing offers a simple, tunable compromise in this particular regime.
      </p>

    </div>
    <div class="margin-right-block" style="transform: translate(0%, -100%);">
      This section details all the methods used in the project. We detail quantitatively and qualitatively how each method performed.
    </div>
  </div>

  <!-- CONCLUSION -->
  <div class="content-margin-container" id="conclusion">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <h1>Conclusion</h1>

      <p>
        In this project, we studied loss functions when dealing with class imbalance in medical image classification. We compared six different training methods while using a ViT as our model on the RSNA pneumonia dataset. We examined basic NLL, class-weighted NLL, Focal Loss, a mixed loss that blends class-weighted NLL with Focal Loss, and two types of data-level rebalancing, namely oversampling and minority only augmentation.
      </p>

      Our results support three main conclusions:
      <ol>
        <li><b>Rebalancing already fixes a large fraction of the problem:</b><br>
          The baseline model that we used achieves solid accuracy but almost always predicts non-pneumonia. Adding even simple class weights or an oversampling procedure can move pneumonia recall from near zero to 30-70% without large drops in overall performance.</li>
        
        <li><b>Focal-loss-like objectives improve minority class performance but require additional hyperparameter tuning:</b><br>
          Focal Loss and our CAF mixed loss both provide additional improvements over basic class weighting, especially in pneumonia F1. However, these gains come with extra hyperparameters that researchers must tune. Naively chosen alpha, gamma, or mixing coefficients can over focus on noisy, mislabeled examples or destabilize training, especially in smaller datasets.</li>
        
        <li><b>CAF-style mixing is a practical compromise:</b><br>
          Our CAF-style loss offers a simple way to combine focal focusing and class imbalance. Through mixing a class-weighted NLL loss term with a focal loss term using a single lambda hyperparameter, we obtain a “balanced focal” objective whose strength is controlled by one parameter. In our experiments, a lambda value of 0.8, which may be a good default for imbalance tasks, results in strong performance across metrics.</li>
      </ol>

      <p>
        More broadly, this project shows that we should evaluate models not only by overall ROC-AUC but also by minority-class sensitivity for clinical screening tasks. Also, the project shows how confusion matrices can change under different imbalance strategies which have different levels of robustness to realistic class prevalence shifts across training and deployment.
      </p>

      <p>
        Our recommendation in a practical standpoint is to start with class-weighted NLL and minority-focused augmentation which solve a large portion of the imbalance problems with relatively low complexity. After adding the above, we recommend layering in focal or CAF-style losses if the application demands sensitivity on rare, high-risk findings and if you can afford compute power for additional hyperparameter tuning. Lastly, we note the importance to inspect not just ROC-AUC but also confusion matrices and per-class metrics before deploying a model in a clinical workflow.
      </p>

      <p>
        In future work, we plan to add calibration metrics (such as Expected Calibration Error and Brier score) to understand how each loss affects probability estimates, but also explore other combinations of CAF-style losses with group-aware objectives and distributionally robust training for fairer performance across patient subgroups. Altogether, our experiments suggest that loss functions are a powerful but often underused method for dealing with class imbalance in medical imaging, and that relatively simple, ViT-compatible variants can offer substantial improvements over naive baselines.
      </p>
    </div>
    <div class="margin-right-block">
      Taken together, our experiments suggest that loss functions are a powerful but often underused lever for dealing with class imbalance in medical imaging—and that relatively simple, ViT-compatible variants can offer substantial improvements over naive baselines.
    </div>
  </div>

  <!-- REFERENCES -->
  <div class="content-margin-container" id="citations">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
      <div class="citation" id="references" style="height:auto"><br>
        <span style="font-size:16px">References:</span><br><br>
        <a id="ref_1"></a>[1] Dosovitskiy et al., <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>, 2020.<br><br>
        <a id="ref_2"></a>[2] Lin et al., <a href="https://arxiv.org/abs/1708.02002">Focal Loss for Dense Object Detection</a>, 2017.<br><br>
        <a id="ref_3"></a>[3] Kaggle RSNA Pneumonia Detection Challenge, <a href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge">competition page</a>.<br><br>
        <a id="ref_4"></a>[4] Selected papers and blog posts on class imbalance and reweighting in medical imaging, including work on rebalancing chest X-ray datasets for pneumonia and TB screening.<br><br>
      </div>
    </div>
    <div class="margin-right-block">
    </div>
  </div>

</body>

</html>
